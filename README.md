# Calculus for Machine Learning

## Overview
This repository contains Jupyter notebooks demonstrating how **calculus concepts are applied in machine learning and optimization**.  
The notebooks combine mathematical intuition, visualizations, and Python implementations to connect theory with practical ML systems.

---

## Repository Structure

### 1. Gradients and Gradient Descent
- First-order derivatives  
- Direction of steepest ascent and descent  
- 1D and 2D gradient descent implementation  
- Effect of learning rate on convergence  
- Visualization of optimization paths  

### 2. Hessians, Curvature, and Convexity
- Second-order derivatives and Hessian matrix  
- Curvature analysis and eigenvalues  
- Convex vs non-convex functions  
- Stability and convergence behavior  
- Surface and contour visualizations  

### 3. Jacobians, Chain Rule, and Partial Derivatives
- Multivariable derivatives  
- Chain rule and its role in backpropagation  
- Jacobian matrices for vector-valued functions  
- Connection to neural network training and parameter updates  

---

## Tools & Libraries
- Python  
- NumPy  
- Matplotlib  


---

## Learning Objectives
- Understand gradients and their role in optimization  
- Analyze curvature using Hessians  
- Distinguish convex vs non-convex functions  
- Apply chain rule and Jacobians in multivariable systems  
- Relate calculus concepts to neural network training and loss minimization  

---

## Relevance to Machine Learning
Modern machine learning algorithms rely heavily on calculus.  
Gradients guide parameter updates, Hessians describe curvature and stability, and the chain rule enables backpropagation in neural networks.  
This repository demonstrates how these mathematical tools directly influence optimization and model behavior.

---


## Summary
This project provides a structured exploration of **calculus for optimization and learning systems**, combining theory with implementation to build both analytical understanding and practical intuition for machine learning.
